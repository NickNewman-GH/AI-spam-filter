# AI-spam-filter
## Описание и структура
Данный проект является зачётным по предмету "Искусственный интелект"

Он состоит из двух частей - двух отдельных ноутбуков

Также, здесь есть две папки:
* data - в ней находятся данные и readme о них
* models - в неё сохранялись обученные модели
## Работа программы
### [model-learning.ipynb](https://github.com/NickNewman-GH/AI-spam-filter/blob/main/model_learning.ipynb)
1. Прописываем import для всех нужных библиотек и модулей
2. Загружаем коллекции слов, которые часто попадаются и не сильно должны влять на модель
3. Разархивируем данные, записываем данные в файл для последующего открытия через pandas для быстроты и удобства работы
4. Открываем файл с данными через pandas
5. Разделяем данные на обучающую и тестовую выборку, для последующего обучения и тестов модели соответственно. Размеры 80% и 20% от общего датасета
6. Функция токенизатора принимает на вход строку и делает следующие вещи: убирает все символы, кроме букв, преобразует все буквы к нижнему регистру, убирает все слова из набора stopwords. Также, функция участвует в преобразовании датасета в матрицу
7. CountVectorizer преобразует датасет в матрицу подсчета токенов, TfidfTransformer преобразует эту матрицу в нормализованое представление tf/tfidf и, наконец, на этой матрице используем классификатор SGDClassifier
8. Тестируем модель на тестовых данных
9. Сохраняем классификатор и векторизатор для дальнейшего использования
### [model-using.ipynb](https://github.com/NickNewman-GH/AI-spam-filter/blob/main/model_learning.ipynb)
1. Прописываем import для всех нужных библиотек и модулей
2. Загружаем коллекции слов, которые часто попадаются и не сильно должны влять на модель
3. Объявляем функцию токенизатора для использования в векторизаторе
4. Открываем обученную модель и векторизатор и кладем в переменные
5. Модель будет принимать текст и давать предсказания о том, спам или не спам введённое сообщение
6. Дополнительная проверка модели на всех данных. Также, можно использовать как шаблон для ввода новых данных и проверки на них ;)
